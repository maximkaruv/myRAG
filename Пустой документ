import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Any
from rag_engine import rag_engine

app = FastAPI(title="Simple RAG System with Metadata")

# Models for API
class DocumentCreate(BaseModel):
    title: str
    content: str
    role: str
    metadata: Any

class DocumentResponse(BaseModel):
    title: str
    content: str
    role: str
    metadata: Any

class Query(BaseModel):
    question: str

class Answer(BaseModel):
    answer: str
    context: List[DocumentResponse]

# Endpoints
@app.post("/documents", response_model=dict)
async def add_document(doc: DocumentCreate):
    try:
        doc_id = rag_engine.add_document(
            title=doc.title,
            content=doc.content,
            role=doc.role,
            metadata=doc.metadata
        )
        return {"status": "success", "document_id": doc_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents", response_model=List[DocumentResponse])
async def list_documents():
    return rag_engine.documents

@app.delete("/documents/{doc_id}")
async def delete_document(doc_id: int):
    success = rag_engine.delete_document(doc_id)
    if not success:
        raise HTTPException(status_code=404, detail="Document not found")
    return {"status": "success", "message": f"Document {doc_id} deleted"}

@app.post("/ask", response_model=Answer)
async def ask_question(query: Query):
    try:
        context_docs = rag_engine.search(query.question)
        answer = rag_engine.generate_answer(query.question)
        return Answer(answer=answer, context=context_docs)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
    
    
    
import faiss
import numpy as np
import json
import os
from openai import OpenAI
from config import settings
from typing import Any

class RAGEngine:
    def __init__(self):
        # Clients
        self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY, base_url=settings.OPENAI_BASE_URL)
        self.deepseek_client = OpenAI(api_key=settings.DEEPSEEK_API_KEY, base_url=settings.DEEPSEEK_BASE_URL)
        
        # Vector DB
        self.dimension = settings.VECTOR_DIMENSION
        self.index = faiss.IndexFlatL2(self.dimension)
        self.documents = []  # Store dicts with text and metadata
        
        # Load existing data if available
        self.load_storage()

    def get_embedding(self, text: str):
        response = self.openai_client.embeddings.create(
            input=text,
            model=settings.EMBEDDING_MODEL
        )
        # Keeping user's cost logging logic
        cost = response.usage.total_tokens / 1_000_000 * 0.02 * 80
        print(f"OpenAI embeddings cost: {cost}, tokens: {response.usage.total_tokens}")
        
        return np.array(response.data[0].embedding).astype('float32')

    def add_document(self, title: str, content: str, role: str, metadata: Any):
        doc_data = {
            "title": title,
            "content": content,
            "role": role,
            "metadata": metadata
        }
        # Embed the content (or a combination of metadata and content)
        embedding_text = f"Title: {title}\nContent: {content}"
        embedding = self.get_embedding(embedding_text)
        
        self.index.add(np.array([embedding]))
        self.documents.append(doc_data)
        
        self.save_storage()
        return len(self.documents) - 1

    def delete_document(self, doc_id: int):
        if 0 <= doc_id < len(self.documents):
            self.documents.pop(doc_id)
            self.rebuild_index()
            self.save_storage()
            return True
        return False

    def rebuild_index(self):
        self.index = faiss.IndexFlatL2(self.dimension)
        if self.documents:
            embeddings = []
            for doc in self.documents:
                embedding_text = f"Title: {doc['title']}\nContent: {doc['content']}"
                embeddings.append(self.get_embedding(embedding_text))
            self.index.add(np.array(embeddings))

    def save_storage(self):
        # Save FAISS index
        faiss.write_index(self.index, settings.INDEX_PATH)
        # Save documents as JSON
        with open(settings.DOCS_PATH, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)

    def load_storage(self):
        if os.path.exists(settings.INDEX_PATH) and os.path.exists(settings.DOCS_PATH):
            self.index = faiss.read_index(settings.INDEX_PATH)
            with open(settings.DOCS_PATH, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
            print(f"Loaded {len(self.documents)} documents from storage.")

    def search(self, query: str, k: int = 3):
        if not self.documents:
            return []
        query_embedding = self.get_embedding(query)
        distances, indices = self.index.search(np.array([query_embedding]), k)
        
        results = []
        for idx in indices[0]:
            if idx != -1 and idx < len(self.documents):
                results.append(self.documents[idx])
        return results

    def format_context(self, docs):
        context_parts = []
        for doc in docs:
            part = (
                f"--- Document ---\n"
                f"Title: {doc['title']}\n"
                f"Content: {doc['content']}\n"
            )
            context_parts.append(part)
        return "\n".join(context_parts)

    def generate_answer(self, query: str):
        context_docs = self.search(query)
        context = self.format_context(context_docs)
        
        prompt = f"Context:\n{context}\n\nQuestion: {query}"
        
        response = self.deepseek_client.chat.completions.create(
            model=settings.GENERATIVE_MODEL,
            messages=[
                {"role": "system", "content": settings.SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        
        # Keeping user's cost logging logic
        cost_input = response.usage.prompt_tokens / 1_000_000 * 0.028 * 80
        cost_output = response.usage.total_tokens / 1_000_000 * 0.42 * 80
        print(f"Deepseek cost input: {cost_input}, tokens: {response.usage.prompt_tokens}")
        print(f"Deepseek cost output: {cost_output}, tokens: {response.usage.total_tokens}")
        
        return response.choices[0].message.content

rag_engine = RAGEngine()



import os
from pydantic import Field
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # API Keys
    OPENAI_API_KEY: str = Field(default="your_openai_api_key", env="OPENAI_API_KEY")
    DEEPSEEK_API_KEY: str = Field(default="your_deepseek_api_key", env="DEEPSEEK_API_KEY")
    
    # Base URLs
    OPENAI_BASE_URL: str = Field(default="https://api.openai.com/v1", env="OPENAI_BASE_URL")
    DEEPSEEK_BASE_URL: str = Field(default="https://api.deepseek.com", env="DEEPSEEK_BASE_URL")
    
    # Models
    EMBEDDING_MODEL: str = Field(default="text-embedding-3-small", env="EMBEDDING_MODEL")
    GENERATIVE_MODEL: str = Field(default="deepseek-chat", env="GENERATIVE_MODEL")
    
    # RAG Settings
    SYSTEM_PROMPT: str = "You are a helpful assistant. Use the provided context (including metadata like title, source, and role) to answer the user's question. If you don't know the answer, say that you don't know."
    VECTOR_DIMENSION: int = 1536  # Default for text-embedding-3-small
    
    # Storage Settings
    INDEX_PATH: str = "database/faiss_index.bin"
    DOCS_PATH: str = "database/documents.json"
    
    class Config:
        env_file = ".env"

settings = Settings()

